# Issue FEAT-002: LLM-Optimized Output Format

## Description

Create a dedicated output format optimized for LLM token efficiency while preserving critical information needed for code generation. This format reduces token usage by 70-90% compared to full JSON output.

## Problem Details

### Current State

**Standard JSON output is token-heavy:**

```bash
$ pydocq json.dump
```

**Current output (~450 tokens):**
```json
{
  "path": "json.dump",
  "type": "function",
  "module_path": "json",
  "signature": {
    "parameters": [
      {
        "name": "obj",
        "kind": "POSITIONAL_OR_KEYWORD",
        "annotation": "Any",
        "default": null
      },
      {
        "name": "fp",
        "kind": "POSITIONAL_OR_KEYWORD",
        "annotation": "SupportsWrite[str]",
        "default": null
      },
      {
        "name": "skipkeys",
        "kind": "KEYWORD_ONLY",
        "annotation": "bool",
        "default": "False"
      },
      {
        "name": "ensure_ascii",
        "kind": "KEYWORD_ONLY",
        "annotation": "bool",
        "default": "True"
      },
      {
        "name": "check_circular",
        "kind": "KEYWORD_ONLY",
        "annotation": "bool",
        "default": "True"
      },
      {
        "name": "allow_nan",
        "kind": "KEYWORD_ONLY",
        "annotation": "bool",
        "default": "True"
      },
      {
        "name": "cls",
        "kind": "KEYWORD_ONLY",
        "annotation": "type[JSONEncoder] | None",
        "default": "null"
      },
      {
        "name": "indent",
        "kind": "KEYWORD_ONLY",
        "annotation": "int | str | None",
        "default": "null"
      },
      {
        "name": "separators",
        "kind": "KEYWORD_ONLY",
        "annotation": "tuple[str, str] | None",
        "default": "null"
      },
      {
        "name": "default",
        "kind": "KEYWORD_ONLY",
        "annotation": "Callable[[Any], Any] | None",
        "default": "null"
      },
      {
        "name": "sort_keys",
        "kind": "KEYWORD_ONLY",
        "annotation": "bool",
        "default": "False"
      }
    ],
    "return_type": "None"
  },
  "docstring": {
    "docstring": "Serialize obj as a JSON formatted stream to fp (a .write()-supporting file-like object).\n\nIf skipkeys is true, then dict keys that are not of a basic type (str, int, float, bool, None) will be skipped instead of raising a TypeError.",
    "length": 350
  }
}
```

### Problems for AI Agents

| Problem | Impact | Severity |
|---------|--------|----------|
| **Token Waste** | 70-90% of tokens are noise for LLMs | High |
| **Context Window** | Limits how many functions can be in context | High |
| **Cost** | More tokens = higher API costs | High |
| **Speed** | Larger context = slower generation | Medium |
| **Information Overload** | Too much detail obscures key info | Medium |

### What AI Agents Actually Need

For code generation, AI agents need:
1. **Function purpose** (1 sentence)
2. **Key parameters** (commonly used ones, not all)
3. **Typical usage example**
4. **Common pitfalls**
5. **Alternatives**

They don't need:
- All 11 parameters for simple use cases
- Full docstring (can be summarized)
- Parameter kinds (POSITIONAL_OR_KEYWORD is obvious)
- Return type for void functions

## Impact Assessment

| Impact Type | Severity | Description |
|-------------|----------|-------------|
| Cost Reduction | 游릭 High | 70-90% token savings |
| Performance | 游릭 High | Faster generation with smaller context |
| Usability | 游릭 High | Easier for agents to extract key info |
| Adoption | 游리 Medium | Requires agents to use new format |
| Backward Compatibility | 游릭 Low | New format doesn't break existing |

## Recommended Implementation

### Step 1: Add LLM Formatter

```python
# pydocq/analyzer/output_formats.py

def format_llm(inspected: InspectedElement) -> dict:
    """Format output optimized for LLM consumption.

    Focus on token efficiency while preserving critical information:
    - Summarize docstring to 1 sentence
    - Extract key parameters (commonly used)
    - Provide concise usage example
    - Highlight common pitfalls
    - Suggest alternatives

    Args:
        inspected: Element to format

    Returns:
        LLM-optimized dictionary
    """
    # Extract key info
    signature = inspected.signature
    docstring = inspected.docstring or ""

    # Summarize docstring (first sentence or generated)
    summary = _summarize_docstring(docstring)

    # Identify key parameters (positional + commonly used keyword)
    key_params = _extract_key_params(signature.parameters)

    # Generate or extract example
    example = _generate_example(inspected)

    # Extract common pitfalls from docstring
    pitfalls = _extract_pitfalls(docstring)

    # Estimate token count
    token_count = _estimate_tokens({
        "summary": summary,
        "key_params": key_params,
        "example": example
    })

    return {
        "path": inspected.path,
        "type": inspected.element_type.value,
        "summary": summary,
        "key_params": key_params,
        "common_usage": _get_common_usage(inspected),
        "example_concise": example,
        "common_pitfalls": pitfalls,
        "alternatives": _get_alternatives(inspected),
        "token_count": token_count,
    }


def _summarize_docstring(docstring: str) -> str:
    """Summarize docstring to one sentence.

    Args:
        docstring: Full docstring

    Returns:
        Single sentence summary
    """
    if not docstring:
        return "No description available."

    # Split by sentences and take first
    sentences = docstring.split('.')
    if sentences:
        first = sentences[0].strip()
        # Remove common prefixes
        first = first.lstrip(' *\n')
        if len(first) > 100:
            # Truncate if too long
            first = first[:97] + "..."
        return first + '.'

    return "Function or method."


def _extract_key_params(parameters: list) -> list[dict]:
    """Extract key parameters from signature.

    Priority:
    1. Positional or positional-or-keyword parameters (required)
    2. Parameters with common defaults (None, False, True, 0)
    3. Skip obscure keyword-only params

    Args:
        parameters: List of parameter dicts

    Returns:
        List of key parameter dicts
    """
    key_params = []

    for param in parameters:
        # Skip if not important
        kind = param.get("kind", "")

        # Always include positional parameters
        if kind in ["POSITIONAL_ONLY", "POSITIONAL_OR_KEYWORD"]:
            key_params.append({
                "name": param["name"],
                "type": param.get("annotation", "Any"),
                "required": param.get("default") is None
            })

        # Include common keyword parameters
        elif kind == "KEYWORD_ONLY":
            default = param.get("default")
            if default in [None, "None", "False", "True", "0", "[]", "{}"]:
                key_params.append({
                    "name": param["name"],
                    "type": param.get("annotation", "Any"),
                    "default": default
                })

    return key_params


def _generate_example(inspected: InspectedElement) -> str:
    """Generate or extract usage example.

    Args:
        inspected: Element to generate example for

    Returns:
        Concise usage example string
    """
    # Check if SDK has example
    if inspected.sdk_metadata and "example" in inspected.sdk_metadata:
        return inspected.sdk_metadata["example"].get("code", "")

    # Generate simple example
    path_parts = inspected.path.split(".")

    if inspected.element_type == ElementType.FUNCTION:
        func_name = path_parts[-1]
        params = inspected.signature.parameters

        # Build simple call
        required_params = [
            p["name"] for p in params
            if p.get("kind") in ["POSITIONAL_ONLY", "POSITIONAL_OR_KEYWORD"]
            and p.get("default") is None
        ]

        if required_params:
            args = ", ".join(required_params[:3])  # Max 3 args
            return f"{func_name}({args})"

    return f"# See {inspected.path} documentation"


def _extract_pitfalls(docstring: str) -> list[str]:
    """Extract common pitfalls from docstring.

    Args:
        docstring: Docstring to search

    Returns:
        List of common pitfalls
    """
    pitfalls = []
    doc_lower = docstring.lower()

    # Common warning patterns
    warning_patterns = [
        (r"warning[s]?:", "Warning"),
        (r"note[s]?:", "Note"),
        (r"caution:", "Caution"),
        (r"important:", "Important"),
    ]

    # Extract warnings
    import re
    for pattern, label in warning_patterns:
        matches = re.finditer(pattern, doc_lower)
        for match in matches:
            # Get next sentence after warning
            start = match.end()
            sentence_match = re.search(r'[A-Z][^.]*\.?', docstring[start:start+100])
            if sentence_match:
                pitfalls.append(f"{label}: {sentence_match.group().strip()}")

    return pitfalls[:3]  # Max 3 pitfalls


def _get_common_usage(inspected: InspectedElement) -> str:
    """Get common usage description.

    Args:
        inspected: Element to describe

    Returns:
        Common usage string
    """
    # Check SDK metadata
    if inspected.sdk_metadata and "category" in inspected.sdk_metadata:
        category = inspected.sdk_metadata["category"]
        return f"Used for {category}"

    # Infer from docstring
    docstring = inspected.docstring or ""
    if "serialize" in docstring.lower():
        return "Serialization"
    elif "parse" in docstring.lower() or "read" in docstring.lower():
        return "Data reading/parsing"
    elif "write" in docstring.lower():
        return "Data writing"
    elif "validate" in docstring.lower():
        return "Validation"

    return "General purpose"


def _get_alternatives(inspected: InspectedElement) -> list[dict]:
    """Suggest alternative functions.

    Args:
        inspected: Element to find alternatives for

    Returns:
        List of alternative suggestions
    """
    alternatives = []

    # Check SDK metadata
    if inspected.sdk_metadata and "see_also" in inspected.sdk_metadata:
        for ref in inspected.sdk_metadata["see_also"]:
            alternatives.append({
                "path": ref,
                "reason": "See also"
            })

    # Hard-code common alternatives for stdlib
    path = inspected.path

    # json.dump alternatives
    if path == "json.dump":
        alternatives.append({
            "path": "json.dumps",
            "reason": "Returns string instead of writing to file"
        })
        alternatives.append({
            "path": "pickle.dump",
            "reason": "For Python-specific objects (not JSON)"
        })

    return alternatives


def _estimate_tokens(data: dict) -> int:
    """Estimate token count for output.

    Args:
        data: Dictionary to estimate

    Returns:
        Estimated token count
    """
    # Rough estimate: ~4 characters per token
    text = str(data)
    return len(text) // 4
```

### Step 2: Add CLI Option

```python
# pydocq/cli.py

@app.command()
def query(
    target: str,
    format: str = Option("json", "--format", "-f", help="Output format (json, raw, signature, markdown, yaml, llm)"),
    # ... existing options
) -> None:
    """Query Python package documentation."""
    # ... existing code

    if format == "llm":
        from pydocq.analyzer.output_formats import format_llm
        output_dict = format_llm(inspected)
        sys.stdout.write(json.dumps(output_dict, indent=2))
        return
```

### Step 3: Add `--for-ai` Flag (Convenience)

```python
# pydocq/cli.py

@app.command()
def query(
    target: str,
    for_ai: bool = Option(False, "--for-ai", help="Optimize output for AI agent consumption"),
    # ... existing options
) -> None:
    """Query Python package documentation."""
    # ... existing code

    if for_ai:
        # Enable all AI-optimized options
        from pydocq.analyzer.output_formats import format_llm
        output_dict = format_llm(inspected)
        # Include SDK metadata if available
        if inspected.sdk_metadata:
            output_dict["sdk_metadata"] = inspected.sdk_metadata
        # Include source location
        if inspected.source_location:
            output_dict["source"] = inspected.source_location
        sys.stdout.write(json.dumps(output_dict, indent=2))
        return
```

## Output Format Examples

### Example 1: Simple Function

**Input:**
```bash
$ pydocq json.dumps --format llm
```

**Output (~50 tokens vs 450):**
```json
{
  "path": "json.dumps",
  "type": "function",
  "summary": "Serialize Python object to JSON string.",
  "key_params": [
    {"name": "obj", "type": "Any", "required": true},
    {"name": "indent", "type": "int | None", "default": "null"}
  ],
  "common_usage": "Convert Python object to JSON string",
  "example_concise": "json.dumps({'key': 'value'}, indent=2)",
  "common_pitfalls": [
    "Note: obj must be JSON-serializable (dict, list, str, int, float, bool, None)"
  ],
  "alternatives": [
    {"path": "json.dump", "reason": "Write directly to file instead of returning string"},
    {"path": "pickle.dumps", "reason": "For Python-specific objects"}
  ],
  "token_count": 48
}
```

### Example 2: Complex Function

**Input:**
```bash
$ pydocq pandas.DataFrame.merge --format llm
```

**Output (~80 tokens vs 1200):**
```json
{
  "path": "pandas.DataFrame.merge",
  "type": "method",
  "summary": "Merge DataFrame or named Series objects with a database-style join.",
  "key_params": [
    {"name": "right", "type": "DataFrame | Series", "required": true},
    {"name": "on", "type": "str | list", "required": false},
    {"name": "how", "type": "str", "default": "'inner'"}
  ],
  "common_usage": "Combine DataFrames using database-style joins",
  "example_concise": "df1.merge(df2, on='key', how='inner')",
  "common_pitfalls": [
    "Warning: Duplicate columns will have suffixes ('_x', '_y')",
    "Note: Memory intensive for very large DataFrames"
  ],
  "alternatives": [
    {"path": "pandas.DataFrame.join", "reason": "Simpler index-based merging"},
    {"path": "pandas.concat", "reason": "Stack DataFrames along an axis"}
  ],
  "token_count": 78
}
```

### Example 3: Class

**Input:**
```bash
$ pydocq json.JSONDecoder --format llm
```

**Output (~60 tokens vs 800):**
```json
{
  "path": "json.JSONDecoder",
  "type": "class",
  "summary": "Simple JSON decoder class for parsing JSON strings.",
  "key_params": [
    {"name": "object_hook", "type": "callable", "default": "null"},
    {"name": "parse_float", "type": "callable", "default": "null"}
  ],
  "common_usage": "Custom JSON deserialization with hooks",
  "example_concise": "decoder = json.JSONDecoder(); decoder.decode('{\"a\": 1}')",
  "common_pitfalls": [
    "Note: For simple cases, use json.loads() instead"
  ],
  "alternatives": [
    {"path": "json.loads", "reason": "Direct deserialization (simpler)"},
    {"path": "json.JSONEncoder", "reason": "For encoding objects to JSON"}
  ],
  "token_count": 62
}
```

## Testing

### Unit Tests

```python
# tests/test_llm_format.py
import pytest
from pydocq.analyzer.output_formats import format_llm

class TestLLMFormat:
    """Test suite for LLM-optimized output format."""

    def test_token_reduction_simple(self):
        """Test that token count is reduced for simple functions."""
        # Create inspected element
        inspected = self._create_mock_function()

        # Get standard format
        standard_output = format_json(inspected)
        standard_tokens = len(str(standard_output)) // 4

        # Get LLM format
        llm_output = format_llm(inspected)
        llm_tokens = llm_output["token_count"]

        # LLM format should be significantly smaller
        assert llm_tokens < standard_tokens * 0.3

    def test_summary_extraction(self):
        """Test that docstring is summarized."""
        inspected = self._create_mock_function(
            docstring="Serialize obj as a JSON formatted stream. "
                     "This is a long docstring with many details. "
                     "It goes on and on. This is extra information."
        )

        output = format_llm(inspected)

        # Should be truncated
        assert len(output["summary"]) < 100
        assert "Serialize" in output["summary"]

    def test_key_params_extraction(self):
        """Test that key parameters are identified."""
        inspected = self._create_mock_function(
            parameters=[
                {"name": "obj", "kind": "POSITIONAL_OR_KEYWORD", "default": None},
                {"name": "fp", "kind": "POSITIONAL_OR_KEYWORD", "default": None},
                {"name": "skipkeys", "kind": "KEYWORD_ONLY", "default": "False"},
                {"name": "ensure_ascii", "kind": "KEYWORD_ONLY", "default": "True"},
                {"name": "obscure_param", "kind": "KEYWORD_ONLY", "default": "None"}
            ]
        )

        output = format_llm(inspected)

        # Should have key params
        assert len(output["key_params"]) >= 2
        assert any(p["name"] == "obj" for p in output["key_params"])
        assert any(p["name"] == "skipkeys" for p in output["key_params"])

    def test_example_generation(self):
        """Test that examples are generated."""
        inspected = self._create_mock_function(
            path="json.dump",
            parameters=[
                {"name": "obj", "kind": "POSITIONAL_OR_KEYWORD", "default": None},
                {"name": "fp", "kind": "POSITIONAL_OR_KEYWORD", "default": None}
            ]
        )

        output = format_llm(inspected)

        # Should have example
        assert "example_concise" in output
        assert "json.dump" in output["example_concise"]

    def test_alternatives_suggestion(self):
        """Test that alternatives are suggested."""
        inspected = self._create_mock_function(path="json.dump")

        output = format_llm(inspected)

        # Should have alternatives
        assert len(output["alternatives"]) > 0
        assert any(a["path"] == "json.dumps" for a in output["alternatives"])

    def test_pitfalls_extraction(self):
        """Test that pitfalls are extracted."""
        inspected = self._create_mock_function(
            docstring="Serialize object. Warning: obj must be serializable."
        )

        output = format_llm(inspected)

        # Should extract warning
        assert len(output["common_pitfalls"]) >= 0

    def _create_mock_function(self, path="test.func", docstring="Test function.", parameters=None):
        """Create mock InspectedElement for testing."""
        from pydocq.analyzer.inspector import InspectedElement
        from pydocq.utils.type_detection import ElementType
        from dataclasses import dataclass

        @dataclass
        class MockSignature:
            parameters: list
            return_type: str = "None"

        if parameters is None:
            parameters = [
                {"name": "x", "kind": "POSITIONAL_OR_KEYWORD", "default": None}
            ]

        return InspectedElement(
            path=path,
            obj=lambda: None,
            element_type=ElementType.FUNCTION,
            signature=MockSignature(parameters=parameters),
            docstring=docstring,
            source_location=None,
            sdk_metadata=None
        )
```

## Implementation Priority

1. **Immediate (P0):** Implement `format_llm()` function
2. **Immediate (P0):** Add `--format llm` CLI option
3. **Short-term (P1):** Add `--for-ai` convenience flag
4. **Testing (P1):** Add comprehensive test suite
5. **Documentation (P1):** Update README with examples

## Migration Plan

### Phase 1: Core Implementation (Week 1)
- [ ] Implement `format_llm()` in `output_formats.py`
- [ ] Add `--format llm` option to CLI
- [ ] Implement helper functions (_summarize, _extract_key_params, etc.)
- [ ] Add token estimation

### Phase 2: Enhancement (Week 1-2)
- [ ] Implement `--for-ai` flag
- [ ] Add SDK metadata integration
- [ ] Add smart alternatives detection
- [ ] Improve example generation

### Phase 3: Testing (Week 2)
- [ ] Add unit tests for all helpers
- [ ] Add token reduction tests
- [ ] Test on real packages (json, pandas, etc.)
- [ ] Benchmark token savings

### Phase 4: Documentation (Week 2)
- [ ] Update README with llm format examples
- [ ] Add comparison table (tokens saved)
- [ ] Document for AI agent developers

## Benefits

| Benefit | Impact |
|---------|--------|
| **Token Savings** | 70-90% reduction in token usage |
| **Cost Reduction** | Lower API costs for users |
| **Faster Generation** | Smaller context = faster inference |
| **Better Focus** | Agents get key info, not noise |
| **Competitive Edge** | Unique feature for AI tools |

## Related Issues

- [FEAT-003: Combined --for-ai Flag](./010-combined-ai-flag.md)
- [FEAT-001: Expose AST Analysis to CLI](./008-expose-ast-analysis-to-cli.md)

## References

- [LLM Token Counting](https://platform.openai.com/tokenizer)
- [Output Formats Module](../pydocq/analyzer/output_formats.py)

## Checklist

- [ ] Implement `format_llm()` function
- [ ] Implement `_summarize_docstring()` helper
- [ ] Implement `_extract_key_params()` helper
- [ ] Implement `_generate_example()` helper
- [ ] Implement `_extract_pitfalls()` helper
- [ ] Implement `_get_alternatives()` helper
- [ ] Implement `_estimate_tokens()` helper
- [ ] Add `--format llm` option to CLI
- [ ] Add `--for-ai` convenience flag
- [ ] Add unit tests for all functions
- [ ] Add token reduction benchmarks
- [ ] Test on real packages
- [ ] Update README with examples
- [ ] Document for AI agent developers
